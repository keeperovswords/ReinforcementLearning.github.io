<!DOCTYPE html>
<html lang="en-us">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <head>
  <meta charset="UTF-8">
  <title>Reinforcement Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
</head>

  <body>
    <section class="page-header">
  <h1 class="project-name">Reinforcement Learning</h1>
  <h2 class="project-tagline"></h2>
  <!--
  <a href="#" class="btn">View on GitHub</a>
  <a href="#" class="btn">Download .zip</a>
  <a href="#" class="btn">Download .tar.gz</a>
  -->
</section>

    <section class="main-content">
      
      <h2>Eligibility Traces</h2>
<p class="meta">07 Jun 2017</p>

<h1>Aside</h1>
<p>As so far, either <a href="/main/2017/06/07/model-based-method.html">Model-based Methods</a> or <a href="/main/2017/06/07/sample-based-method.html">Model-Free method</a> enables us the find the optimal policy, but not effectively. In this section we’ll use some extra information to make this process much easier.</p>

<h1>Eligibility Traces</h1>
<p>What’s eligibility traces? To answer this question, we can glance the two different views of eligibility traces. From mechanistic view, be short, it’s a extra tracing information with that the learner learns much more effectively than the general methods, this mechanistic view is also noted as <strong>backward view</strong>, depicted in the following figure.</p>
<div class="fig figcenter fighighlight">
  <img src="http://github.com/pages/keeperovswords/ReinforcementLearning.github.io/assets/backwardview.png" width="60%" />
  <div class="figcaption">The mechanistic view of eligibility traces</div>
</div>

<p>For holding this eligibility traces we need a extra memory variable for each state. In a episode we take  an action <script type="math/tex">a</script> under the current <script type="math/tex">\pi</script> for state <script type="math/tex">s</script> and we get a reward <script type="math/tex">R</script> and next state <script type="math/tex">s^\prime</script>. The temporal difference is given by</p>

<script type="math/tex; mode=display">\delta_t = r + \gamma V(s^\prime) - V(s)</script>

<p>Meanwhile the counter of state be accessed will be updated. After it all the state value of each state will be updated with considering the <script type="math/tex">\delta</script> and eligibility variable will be also discounted by the discount factor and eligibility factor <script type="math/tex">\lambda</script>. Once the agent reaches a terminal state or a state with high reward, then this information will be back propagated towards to all the state in terms of the <script type="math/tex">\lambda</script> eligibility trace.  Almost every TD-method can be combined with eligibility trace to work better. The another view is the theoretic view, also as <strong>forward view</strong> noted and depicted in Figure:</p>
<div class="fig figcenter fighighlight">
  <img src="http://github.com/pages/keeperovswords/ReinforcementLearning.github.io/assets/forwardview.png" width="60%" />
  <div class="figcaption">The theoretic view of eligibility traces.</div>
</div>

<p>It’s a bridge between TD-Learning and MC-method. It updates the current state by looking ahead and taking the future states into account. When just only time step is considered, then it turns not surprisingly out that it’s the 1-step TD-method likes <script type="math/tex">v_\pi(S_t)^{(1)} = R_{t+1} + \gamma V(S_{t+1})</script>, whereas the MC-methods performs a backup for estimating state value for each state based on the entire sequence of observed rewards from that state until the end of this episode, formally it looks like in 
\begin{equation}
	v_\pi(S_t) = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{T-t-1}R_T.
\end{equation}</p>

<p>for which we have the sequence events <script type="math/tex">S_t, R_{t+1}, S_{t+1}, R_{t+2}, \dots. R_T</script> and <script type="math/tex">T</script> is the last time step in this episode. Make this equality more generally, we have the “corrected <script type="math/tex">n-</script>step truncated return”:
\begin{equation}
v_\pi(S_t)^{(n)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{n-1}R_{t+n} +\gamma^n V(S_{t+n}).
\end{equation}
It means the current state is truncated after <script type="math/tex">n-</script>steps and corrected by adding the estimating value of <script type="math/tex">n-</script>th next state. In this point of view we can solve either continuous or episodic tasks as well. MC-method is just special case that <script type="math/tex">% <![CDATA[
T-t <n %]]></script>. I used the <script type="math/tex">Q(\lambda)</script> to solve the four-gates problems. As this approach beings, it works exactly likes the Q-learning. Once the agent gets the terminal state, the situation changes. This trace information has been used in next iteration repeatedly and enables the algorithm to get converge much quickly.</p>

<p><b>Dyna-Q</b> is an another TD-learning method that also builds the transition model of environment during the learning. It integrates the planning and control as whole one part. At first the agent tries to learn and to find the optimal policy just like Q-learning does. Meanwhile the model of the environment will be calculated.  Then this model will be used to update value state of all states. Sometimes it works well, if the model is optimistic. But if the environment is stochastic, the performance of this method will be downgraded by the uncertainty of environment, in other words, by the wrong estimated model. For this problem we can reconsider the trade-off between exploration and exploitation and some heuristic strategies.</p>


      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">Reinforcement Learning</a> is maintained by <a href="https://github.com/keeperovswords">Jian Xi</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
</footer>


    </section>

  </body>
</html>
