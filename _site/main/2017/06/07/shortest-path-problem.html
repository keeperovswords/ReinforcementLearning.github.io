<!DOCTYPE html>
<html lang="en-us">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <head>
  <meta charset="UTF-8">
  <title>Reinforcement Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
</head>

  <body>
    <section class="page-header">
  <h1 class="project-name">Reinforcement Learning</h1>
  <h2 class="project-tagline"></h2>
  <a href="#" class="btn">View on GitHub</a>
  <a href="#" class="btn">Download .zip</a>
  <a href="#" class="btn">Download .tar.gz</a>
</section>

    <section class="main-content">
      
      <h2>Shortest Path Problem</h2>
<p class="meta">07 Jun 2017</p>

<p><strong>Reinforcement Learning</strong> is another machine learning algorithm in contrast to models like supervised learning and unsupervised learning. In this problem settings we have an <strong>agent</strong> and want to achieve some goal. Usually we can not tell the agent how to achieve our goal directly in the environment, in which it i.e. moves, gets some feedbacks after taking an action, for sometimes we do not known the dynamics of this environment or even the environment in this environment is non-deterministic, say, assuming we have an agent to move toward to a target, with 0.8 probability the robot will move correctly as we expecte and with 0.2 the robot will turn a little from left etc.  These feedbacks are called <strong>Reward</strong>. All the training data we use in reinforcement learning is generated by interaction of agent with this environment. Normally the environment can be abstracted as it consists of many <strong>states</strong>, the state, at which the agent terminates, is called <strong>absorb</strong> or <strong>terminal</strong> state. At each time step the agent takes an action and gets a reward. When the agent reaches this absorb state, this <strong>episodic</strong> task is closed. So our problem turns to finding the optimal action sets that maximize the total rewards along time runs. The strategy to used to find those actions is called <strong>policy</strong>, with it the agent can reach the terminal state and get many rewards as it’s able to receive. This agent–environment interaction in reinforcement learning is depicted in Fig:</p>

<p><img src="/assets/mdp.png" alt="The agent–environment interaction in reinforcement learning" class="center-image" /></p>

<p>A state signal that succeeds in retaining all relevant information is said to be <strong>Markov</strong> or it satisfies the <strong>Markov Property</strong>. Let’s formally define the Markov property by considering how environment might response at time <script type="math/tex">t + 1</script> to the action taken at time <script type="math/tex">t</script>. In general case this response may depend on what happened in the past:</p>

<script type="math/tex; mode=display">Pr(R_{t+1} = r, S_{t+1} = s'| S_0, A_0, R_0, \dots, S_{t-1}, A_{t-1}, R_t, S_t, A_t),</script>

<p>for all <script type="math/tex">r, s^\prime</script> and the events <script type="math/tex">S_0, A_0, R_0, \dots, S_{t-1}, A_{t-1}, R_t, S_t, A_t</script>. If the environment responses at time <script type="math/tex">t+1</script> is only depend on the the state and action at time $t$, then this form can be shorted as</p>

<script type="math/tex; mode=display">Pr(Rt+1 =r,St+1 =s′|St,At),</script>

<p>for all <script type="math/tex">r, s^\prime, S_t, A_t</script>, then we say this state signal has the <strong>Markov property</strong>. It enables us to predict the next reward and state at time $t+1$ given the state and action at time $t$. According to this one-step dynamic we can iterate all dynamic history up to now to predict the future dynamics. So the best policy for selecting a action is just as to full this history up to over time changes. Even if the state is not markon, it still can be solved appropriate as to be approximated to a markov state.</p>

<p>A RL task satisfies the Markov property is called <strong>Markov Decison Process</strong> (MDP), it’s a <strong>finite MDP</strong> if the state and action are also finite.
Given any state <script type="math/tex">s</script> and action <script type="math/tex">a</script> at time step <script type="math/tex">t</script>, the one-step dynamics for next state <script type="math/tex">s^\prime</script> in MDP is defined in Eq:</p>

<script type="math/tex; mode=display">p(s^\prime|s, a) = Pr(S_{t+1} =s^\prime|S_t = s, A_t = a).</script>

<p>The expected next reward at <script type="math/tex">s^\prime</script> given current state <script type="math/tex">s</script>, action <script type="math/tex">a</script> is</p>

<script type="math/tex; mode=display">r(s, a, s^\prime) = \mathbb{E}[R_{t+1}|S_t = s, A_t = a, S_{t+1} = s^\prime]</script>

<p>This is the basis of reinforcement learning task. Moreover, in reinforcement learning tasks all the training data is coming from the interaction of the agent with the environment. So we do not need extra training samples and labels for reinforcement learning.</p>



      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">Reinforcement Learning</a> is maintained by <a href="https://github.com/keeperovswords">Jian Xi</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
</footer>


    </section>

  </body>
</html>
