<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-06-12T16:34:20+02:00</updated><id>http://localhost:4000/</id><title type="html">Reinforcement Learning</title><subtitle>The introduction of all basic algorithms in reinforcement learning. Keyword: Robotics, Optimal Control, Function Approximation
</subtitle><author><name>Jian Xi</name></author><entry><title type="html">Sample-based Method</title><link href="http://localhost:4000/main/2017/06/07/sample-based-method.html" rel="alternate" type="text/html" title="Sample-based Method" /><published>2017-06-07T17:21:22+02:00</published><updated>2017-06-07T17:21:22+02:00</updated><id>http://localhost:4000/main/2017/06/07/sample-based-method</id><content type="html" xml:base="http://localhost:4000/main/2017/06/07/sample-based-method.html">&lt;h1&gt;Aside&lt;/h1&gt;
&lt;p&gt;In contrast to &lt;a href=&quot;/main/2017/06/07/model-based-method.html&quot;&gt;Model-based Methods&lt;/a&gt; such as PI and VI , in this part the &lt;strong&gt;Model Free Methods &lt;/strong&gt;will be considered. For some case we don’t know the dynamics of our environment. It’s better we just interact with the environment to find the control policy.&lt;/p&gt;

&lt;h1&gt;Monte Carlo Method&lt;/h1&gt;
&lt;p&gt;Monte Carlo Method (MC) is a sample-oriented approach. It does not ask for the dynamic directly, it requires only &lt;strong&gt;experience, stimulated sequences&lt;/strong&gt; of states, actions and rewards or it just interacts with the environment in a on-line way. Therefore it solves the RL tasks based on averaging sample returns. Concretely to say, the all state value in a sampling sequence will be updated equally. What if the sample distribution is not good, the policy we be more than we anticipate. For MC method we always need big amount of training data.&lt;/p&gt;

&lt;h1&gt;Temporal-Difference Learning&lt;/h1&gt;
&lt;p&gt;Temporal-Difference Learning (TD-Learning) is a combination of MC-method and dynamic programming (DP). One side it can use the stimulated experience to learn and at the another side it updates estimates based on the part already learned. What’s the temporal difference? Roughly speaking, it’s the state value or state action value between states. The simply TD method has the following form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1} - V(S_t)) \right]&lt;/script&gt;

&lt;p&gt;If we apply this TD method to action values, we get SARSA method,
\begin{equation}
	Q(S_t, A_t)= Q(S_t, A_t) + \alpha \left[  R_{t+1} + \gamma Q(S_{t+1} - Q(S_t)) \right]
\end{equation} which uses only the five tuples &lt;script type=&quot;math/tex&quot;&gt;S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}&lt;/script&gt;. That’s why it’s called SARSA.&lt;/p&gt;

&lt;p&gt;Q-Learning is another kind of TD-Learning. It approximates the optimal &lt;script type=&quot;math/tex&quot;&gt;q_\star&lt;/script&gt; directly by learning the action value function independent the policy being followed. So it’s an off-policy TD control approach. As we said, the most amazing part or TD is it benefits one side from MC-method, it also bootstrap itself by considering the past experiences. Therefore there are two sides should be considered accordingly, the exploration and the exploitation. Exploration focus much more on to explore the unknown states, whereas exploitation thinks much heavily about using the current estimates greedily. For exploration we have i.g. &lt;script type=&quot;math/tex&quot;&gt;\epsilon-&lt;/script&gt; greedy policy. With given exploration probability &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; we select some action in current state randomly and with &lt;script type=&quot;math/tex&quot;&gt;1-\epsilon&lt;/script&gt; we just follow the current policy. In an non-deterministic MDPs play the noise a critical role. Noise means how uncertain than the agent gets a successor state after taking an action in current state. When the noise is quite big in your environment, it’d be quite if you use a better exploration policy.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/qlearner.png&quot; width=&quot;50%&quot; /&gt;
  &lt;img src=&quot;/assets/qlearner_eps.png&quot; width=&quot;50%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;&lt;b&gt;Top:&lt;/b&gt;Q-learning without exploration &lt;b&gt;Bottom:&lt;/b&gt; Q-learning with epsilon-greedy policy&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Here we have two diagrams: the topmost shows the value function without &lt;script type=&quot;math/tex&quot;&gt;\epsilon-&lt;/script&gt; greedy policy and the botton one shows the value function with greedy policy with &lt;script type=&quot;math/tex&quot;&gt;\epsilon = 0.5&lt;/script&gt;.
As it shows, the action value of states are totally different after 100 iterations. Without random policy there are bunch of actions in states that are still undiscovered yet, whose action value are noted as $0.0$.&lt;/p&gt;

&lt;p&gt;With exploration the Q-learning enables us to using the interaction with environment to stretch the optimal policy. But there are still polish points can be found in Q-learning. The Q-learner always back propagates the value state information or shortest path information from current state to the state where it comes from, as depicted as Fig.:&lt;/p&gt;
&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/qpropagation.png&quot; width=&quot;50%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;Q-leaner propagates the &quot;shortest path information&quot; back each time at one time step &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Actually we know from which states we get the current state implictly, so we can use this information to make Q-learning much effective? In next section we will find an answer for it.&lt;/p&gt;</content><author><name>Jian Xi</name></author><summary type="html">Aside In contrast to Model-based Methods such as PI and VI , in this part the Model Free Methods will be considered. For some case we don’t know the dynamics of our environment. It’s better we just interact with the environment to find the control policy.</summary></entry><entry><title type="html">Eligibility Traces</title><link href="http://localhost:4000/main/2017/06/07/eligibility-traces.html" rel="alternate" type="text/html" title="Eligibility Traces" /><published>2017-06-07T16:14:48+02:00</published><updated>2017-06-07T16:14:48+02:00</updated><id>http://localhost:4000/main/2017/06/07/eligibility-traces</id><content type="html" xml:base="http://localhost:4000/main/2017/06/07/eligibility-traces.html">&lt;h1&gt;Aside&lt;/h1&gt;
&lt;p&gt;As so far, either &lt;a href=&quot;/main/2017/06/07/model-based-method.html&quot;&gt;Model-based Methods&lt;/a&gt; or &lt;a href=&quot;/main/2017/06/07/sample-based-method.html&quot;&gt;Model-Free method&lt;/a&gt; enables us the find the optimal policy, but not effectively. In this section we’ll use some extra information to make this process much easier.&lt;/p&gt;

&lt;h1&gt;Eligibility Traces&lt;/h1&gt;
&lt;p&gt;What’s eligibility traces? To answer this question, we can glance the two different views of eligibility traces. From mechanistic view, be short, it’s a extra tracing information with that the learner learns much more effectively than the general methods, this mechanistic view is also noted as &lt;strong&gt;backward view&lt;/strong&gt;, depicted in the following figure.&lt;/p&gt;
&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/backwardview.png&quot; width=&quot;60%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;The mechanistic view of eligibility traces&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;For holding this eligibility traces we need a extra memory variable for each state. In a episode we take  an action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; under the current &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; for state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; and we get a reward &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; and next state &lt;script type=&quot;math/tex&quot;&gt;s^\prime&lt;/script&gt;. The temporal difference is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_t = r + \gamma V(s^\prime) - V(s)&lt;/script&gt;

&lt;p&gt;Meanwhile the counter of state be accessed will be updated. After it all the state value of each state will be updated with considering the &lt;script type=&quot;math/tex&quot;&gt;\delta&lt;/script&gt; and eligibility variable will be also discounted by the discount factor and eligibility factor &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;. Once the agent reaches a terminal state or a state with high reward, then this information will be back propagated towards to all the state in terms of the &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; eligibility trace.  Almost every TD-method can be combined with eligibility trace to work better. The another view is the theoretic view, also as &lt;strong&gt;forward view&lt;/strong&gt; noted and depicted in Figure:&lt;/p&gt;
&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;http://github.com/pages/keeperovswords/ReinforcementLearning.github.io/assets/forwardview.png&quot; width=&quot;60%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;The theoretic view of eligibility traces.&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;It’s a bridge between TD-Learning and MC-method. It updates the current state by looking ahead and taking the future states into account. When just only time step is considered, then it turns not surprisingly out that it’s the 1-step TD-method likes &lt;script type=&quot;math/tex&quot;&gt;v_\pi(S_t)^{(1)} = R_{t+1} + \gamma V(S_{t+1})&lt;/script&gt;, whereas the MC-methods performs a backup for estimating state value for each state based on the entire sequence of observed rewards from that state until the end of this episode, formally it looks like in 
\begin{equation}
	v_\pi(S_t) = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{T-t-1}R_T.
\end{equation}&lt;/p&gt;

&lt;p&gt;for which we have the sequence events &lt;script type=&quot;math/tex&quot;&gt;S_t, R_{t+1}, S_{t+1}, R_{t+2}, \dots. R_T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is the last time step in this episode. Make this equality more generally, we have the “corrected &lt;script type=&quot;math/tex&quot;&gt;n-&lt;/script&gt;step truncated return”:
\begin{equation}
v_\pi(S_t)^{(n)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{n-1}R_{t+n} +\gamma^n V(S_{t+n}).
\end{equation}
It means the current state is truncated after &lt;script type=&quot;math/tex&quot;&gt;n-&lt;/script&gt;steps and corrected by adding the estimating value of &lt;script type=&quot;math/tex&quot;&gt;n-&lt;/script&gt;th next state. In this point of view we can solve either continuous or episodic tasks as well. MC-method is just special case that &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
T-t &lt;n %]]&gt;&lt;/script&gt;. I used the &lt;script type=&quot;math/tex&quot;&gt;Q(\lambda)&lt;/script&gt; to solve the four-gates problems. As this approach beings, it works exactly likes the Q-learning. Once the agent gets the terminal state, the situation changes. This trace information has been used in next iteration repeatedly and enables the algorithm to get converge much quickly.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Dyna-Q&lt;/b&gt; is an another TD-learning method that also builds the transition model of environment during the learning. It integrates the planning and control as whole one part. At first the agent tries to learn and to find the optimal policy just like Q-learning does. Meanwhile the model of the environment will be calculated.  Then this model will be used to update value state of all states. Sometimes it works well, if the model is optimistic. But if the environment is stochastic, the performance of this method will be downgraded by the uncertainty of environment, in other words, by the wrong estimated model. For this problem we can reconsider the trade-off between exploration and exploitation and some heuristic strategies.&lt;/p&gt;</content><author><name>Jian Xi</name></author><summary type="html">Aside As so far, either Model-based Methods or Model-Free method enables us the find the optimal policy, but not effectively. In this section we’ll use some extra information to make this process much easier.</summary></entry><entry><title type="html">Model-based Method</title><link href="http://localhost:4000/main/2017/06/07/model-based-method.html" rel="alternate" type="text/html" title="Model-based Method" /><published>2017-06-07T16:09:00+02:00</published><updated>2017-06-07T16:09:00+02:00</updated><id>http://localhost:4000/main/2017/06/07/model-based-method</id><content type="html" xml:base="http://localhost:4000/main/2017/06/07/model-based-method.html">&lt;p&gt;As mentioned in &lt;a href=&quot;/main/2017/06/07/shortest-path-problem.html&quot;&gt;Shortest Path&lt;/a&gt;
previously, we want to find a optimal policy, so our agent can get much more rewards and few penalty. This policy is acquired by the interaction of the agent with environment. As the agent works, we should use the feedbacks to extract the policy informations as we can. For it we need a mechanism to describes how a state good or bad for the agent is. In other words, what penalty informations exist at states in environment. This policy is given described by the &lt;strong&gt;value function&lt;/strong&gt; accordingly. Let’s clarify the definition of policy. A policy is &lt;strong&gt;mapping&lt;/strong&gt; from each state &lt;script type=&quot;math/tex&quot;&gt;s \in S&lt;/script&gt;, and action &lt;script type=&quot;math/tex&quot;&gt;a \in A&lt;/script&gt;, to the probability &lt;script type=&quot;math/tex&quot;&gt;\pi(s,a)&lt;/script&gt; of taking action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; in state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;. So the value function of a state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; under a policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; denoted as &lt;script type=&quot;math/tex&quot;&gt;v_{\pi(s)}&lt;/script&gt; is a expected return when agent starts from &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; and follows the policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; over times. In MDPs the &lt;strong&gt;value state function&lt;/strong&gt; for policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_{\pi(s)} = \mathbb{E}\left[ G_t|S_t = s \right] = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^{k} R_{k+t+1} | S_t =s \right]&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;Optimal policy&lt;/strong&gt; is defined as:
&lt;script type=&quot;math/tex&quot;&gt;v_\star(s)= \sum_{a} \pi(a|s) \sum_{s^\prime}p(s^\prime|s,a) \left[ r(s, a, s^\prime) + \gamma v_\pi(s^\prime) \right]&lt;/script&gt;,&lt;/p&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;G_t&lt;/script&gt; is the return in &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; the discount rate, which means how much contribute of the future state will be considered in calculating the current state value. This is equation is so-called &lt;strong&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{Bellman\ Equation}&lt;/script&gt;&lt;/strong&gt;. It reflects the relation between the value of state and its successor state. For iterations we can use &lt;strong&gt;back-up&lt;/strong&gt; to make it easier to understand. The 
Backup for &lt;script type=&quot;math/tex&quot;&gt;v_\pi&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;q_\pi&lt;/script&gt; is represented as:&lt;/p&gt;
&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/backup.png&quot; width=&quot;50%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;Back-up diagram at states&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The solid circles mean the state-action pair and empty circles mean state. What Bellman Operator shows us is that the value in state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; is the weighted sum (the second sum operator in equation) of all successor state by the probability of occurring (the first sum operator).&lt;br /&gt;
Correspondingly the Q-value of a state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; taking an action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; under policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is defined as:
&lt;script type=&quot;math/tex&quot;&gt;q_\pi(s,a) = \mathbb{E}\left[ G_t|S_t = s, A_t = a \right]&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t = s, A_t = a \right]&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\sum_{s^\prime}p(s^\prime|s, a) \left[ r(s,a,s^\prime) + \gamma q_\pi(s^\prime, a^\prime) \right]&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;q_\star(s,a) = \sum_{s^\prime}p(s^\prime|s, a) \left[ r(s,a,s^\prime) + \gamma \max_{a^\prime}q_\star(s^\prime, a^\prime) \right],&lt;/script&gt;
which is called &lt;strong&gt;action  value  function&lt;/strong&gt; for policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The goal is to finding the optimal policy that the corresponding function can be maximized greedily. Once the optimal value function is determined, it’s easy to get optimal policy.&lt;/p&gt;

&lt;p&gt;As so far we give an explanation why do we need value function or action value function, in following section I’ll show some concrete algorithms for investigating the optimal policy. All of them share the same property: they are all &lt;strong&gt;tabular methods&lt;/strong&gt;. It means all the state values are stored temporally in tabular data structure.&lt;/p&gt;

&lt;h1&gt;Policy Iteration&lt;/h1&gt;

&lt;p&gt;Policy Iteration (PI) is dynamic programming operator, it’s shown in the following Fig.&lt;/p&gt;
&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/pi.png&quot; width=&quot;25%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;Policy Iteration&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The given policy will be at first evaluated in a very simple way: for &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;, the current state value is backed up. In current state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; its new state value function&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;v(s)\leftarrow \sum_{s^\prime}p(s^\prime|s,\pi(s)) \left[ r(s, \pi(s), s^\pi) + \gamma v(s^\prime) \right]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;will be iteratively calculated.  After it the difference of two function value at current state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; will be compared with some given accuracy parameters till it converges. In policy improvement we select the action that maximizes the current state value like
&lt;script type=&quot;math/tex&quot;&gt;\pi(s) \leftarrow \underset{a}{\operatorname{argmax}} \sum_{s^\prime} p(s^\prime|s,a) \left[ r(s, a,s^\prime) + \gamma v(s^\prime) \right]&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This process repeats iteratively till the whole policy iteration converges. This methods converges in few iterations. But the protracted policy evaluation needs to sweep all next states to evaluate all states. From this point we have Value Iteration.&lt;/p&gt;

&lt;h1&gt;Value Iteration&lt;/h1&gt;
&lt;p&gt;Value Iteration (VI) is a improvement of PI that merges the policy evaluation and improvement in single operator. Just like PI does, it first computer new state value for the current by sweeping all possible successor states. Then it always selects the action that maximizes the current state value. In other words it’s much more greedier than PI. Here we have 3x4 grid world, where “start” indicates the beginning state of this episodic task.&lt;/p&gt;
&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/vi.png&quot; width=&quot;50%&quot; /&gt;
  &lt;img src=&quot;/assets/qvalue.png&quot; width=&quot;50%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;&lt;b&gt;Top:&lt;/b&gt; An example of value iteration afeter 10 iterations. &lt;b&gt;Bottom:&lt;/b&gt; The corresponding q values after 10 iterations.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;There are two absorb state, where the agent will get positive and negative rewards separately. As the Fig. above shows, VI always selects the action in the current iterations that enables the agent get much more rewards. The cell &lt;script type=&quot;math/tex&quot;&gt;3\times2&lt;/script&gt; is much nearer to terminal state &lt;script type=&quot;math/tex&quot;&gt;3\times4&lt;/script&gt; than start state &lt;script type=&quot;math/tex&quot;&gt;3\times1&lt;/script&gt;, but after learning we still found that it’s better from state &lt;script type=&quot;math/tex&quot;&gt;3\times2&lt;/script&gt;  to &lt;script type=&quot;math/tex&quot;&gt;3\times1&lt;/script&gt;, then go along the direction arrows show to the terminal state.&lt;/p&gt;

&lt;p&gt;Either PI and VI has the character in common that they assume the dynamic of the current environment is available. It means i.e. in the grid world the transition probability from each state to its successor state is given. In real word sometimes it’s either expensive to get this dynamics or it’s impossible to get those things. In the following section the samples-based algorithm will be represented.&lt;/p&gt;</content><author><name>Jian Xi</name></author><summary type="html">As mentioned in Shortest Path previously, we want to find a optimal policy, so our agent can get much more rewards and few penalty. This policy is acquired by the interaction of the agent with environment. As the agent works, we should use the feedbacks to extract the policy informations as we can. For it we need a mechanism to describes how a state good or bad for the agent is. In other words, what penalty informations exist at states in environment. This policy is given described by the value function accordingly. Let’s clarify the definition of policy. A policy is mapping from each state , and action , to the probability of taking action in state . So the value function of a state under a policy denoted as is a expected return when agent starts from and follows the policy over times. In MDPs the value state function for policy is defined as</summary></entry><entry><title type="html">Shortest Path Problem</title><link href="http://localhost:4000/main/2017/06/07/shortest-path-problem.html" rel="alternate" type="text/html" title="Shortest Path Problem" /><published>2017-06-07T15:01:22+02:00</published><updated>2017-06-07T15:01:22+02:00</updated><id>http://localhost:4000/main/2017/06/07/shortest-path-problem</id><content type="html" xml:base="http://localhost:4000/main/2017/06/07/shortest-path-problem.html">&lt;p&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt; is another machine learning algorithm in contrast to models like supervised learning and unsupervised learning. In this problem settings we have an &lt;strong&gt;agent&lt;/strong&gt; and want to achieve some goal. Usually we can not tell the agent how to achieve our goal directly in the environment, in which it i.e. moves, gets some feedbacks after taking an action, for sometimes we do not known the dynamics of this environment or even the environment in this environment is non-deterministic, say, assuming we have an agent to move toward to a target, with 0.8 probability the robot will move correctly as we expecte and with 0.2 the robot will turn a little from left etc.  These feedbacks are called &lt;strong&gt;Reward&lt;/strong&gt;. All the training data we use in reinforcement learning is generated by interaction of agent with this environment. Normally the environment can be abstracted as it consists of many &lt;strong&gt;states&lt;/strong&gt;, the state, at which the agent terminates, is called &lt;strong&gt;absorb&lt;/strong&gt; or &lt;strong&gt;terminal&lt;/strong&gt; state. At each time step the agent takes an action and gets a reward. When the agent reaches this absorb state, this &lt;strong&gt;episodic&lt;/strong&gt; task is closed. So our problem turns to finding the optimal action sets that maximize the total rewards along time runs. The strategy to used to find those actions is called &lt;strong&gt;policy&lt;/strong&gt;, with it the agent can reach the terminal state and get many rewards as it’s able to receive. This agent–environment interaction in reinforcement learning is depicted in Fig:&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;http://localhost:4000/assets/mdp.png&quot; width=&quot;20%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;The agent–environment interaction in reinforcement learning&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;A state signal that succeeds in retaining all relevant information is said to be &lt;strong&gt;Markov&lt;/strong&gt; or it satisfies the &lt;strong&gt;Markov Property&lt;/strong&gt;. Let’s formally define the Markov property by considering how environment might response at time &lt;script type=&quot;math/tex&quot;&gt;t + 1&lt;/script&gt; to the action taken at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. In general case this response may depend on what happened in the past:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr(R_{t+1} = r, S_{t+1} = s'| S_0, A_0, R_0, \dots, S_{t-1}, A_{t-1}, R_t, S_t, A_t),&lt;/script&gt;

&lt;p&gt;for all &lt;script type=&quot;math/tex&quot;&gt;r, s^\prime&lt;/script&gt; and the events &lt;script type=&quot;math/tex&quot;&gt;S_0, A_0, R_0, \dots, S_{t-1}, A_{t-1}, R_t, S_t, A_t&lt;/script&gt;. If the environment responses at time &lt;script type=&quot;math/tex&quot;&gt;t+1&lt;/script&gt; is only depend on the the state and action at time $t$, then this form can be shorted as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr(Rt+1 =r,St+1 =s′|St,At),&lt;/script&gt;

&lt;p&gt;for all &lt;script type=&quot;math/tex&quot;&gt;r, s^\prime, S_t, A_t&lt;/script&gt;, then we say this state signal has the &lt;strong&gt;Markov property&lt;/strong&gt;. It enables us to predict the next reward and state at time $t+1$ given the state and action at time $t$. According to this one-step dynamic we can iterate all dynamic history up to now to predict the future dynamics. So the best policy for selecting a action is just as to full this history up to over time changes. Even if the state is not markon, it still can be solved appropriate as to be approximated to a markov state.&lt;/p&gt;

&lt;p&gt;A RL task satisfies the Markov property is called &lt;strong&gt;Markov Decison Process&lt;/strong&gt; (MDP), it’s a &lt;strong&gt;finite MDP&lt;/strong&gt; if the state and action are also finite.
Given any state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; and action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; at time step &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, the one-step dynamics for next state &lt;script type=&quot;math/tex&quot;&gt;s^\prime&lt;/script&gt; in MDP is defined in Eq:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(s^\prime|s, a) = Pr(S_{t+1} =s^\prime|S_t = s, A_t = a).&lt;/script&gt;

&lt;p&gt;The expected next reward at &lt;script type=&quot;math/tex&quot;&gt;s^\prime&lt;/script&gt; given current state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;, action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r(s, a, s^\prime) = \mathbb{E}[R_{t+1}|S_t = s, A_t = a, S_{t+1} = s^\prime]&lt;/script&gt;

&lt;p&gt;This is the basis of reinforcement learning task. Moreover, in reinforcement learning tasks all the training data is coming from the interaction of the agent with the environment. So we do not need extra training samples and labels for reinforcement learning.&lt;/p&gt;</content><author><name>Jian Xi</name></author><summary type="html">Reinforcement Learning is another machine learning algorithm in contrast to models like supervised learning and unsupervised learning. In this problem settings we have an agent and want to achieve some goal. Usually we can not tell the agent how to achieve our goal directly in the environment, in which it i.e. moves, gets some feedbacks after taking an action, for sometimes we do not known the dynamics of this environment or even the environment in this environment is non-deterministic, say, assuming we have an agent to move toward to a target, with 0.8 probability the robot will move correctly as we expecte and with 0.2 the robot will turn a little from left etc. These feedbacks are called Reward. All the training data we use in reinforcement learning is generated by interaction of agent with this environment. Normally the environment can be abstracted as it consists of many states, the state, at which the agent terminates, is called absorb or terminal state. At each time step the agent takes an action and gets a reward. When the agent reaches this absorb state, this episodic task is closed. So our problem turns to finding the optimal action sets that maximize the total rewards along time runs. The strategy to used to find those actions is called policy, with it the agent can reach the terminal state and get many rewards as it’s able to receive. This agent–environment interaction in reinforcement learning is depicted in Fig:</summary></entry></feed>